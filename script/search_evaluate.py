import pyterrier as pt
import pandas as pd
from pyterrier.measures import *

# Initialiser PyTerrier si ce nâ€™est pas dÃ©jÃ  fait
if not pt.started():
    pt.init()

# Charger les requÃªtes
def charger_requetes():
    return pd.DataFrame([
        {"qid": "MB39", "query": "Gaza under attack"},
        {"qid": "MB40", "query": "Military occupation Gaza"},
        {"qid": "MB41", "query": "Israel Genocide Gaza"},
        {"qid": "MB42", "query": "Ceasefire in Gaza"},
        {"qid": "MB43", "query": "Massacres in Gaza"},
        {"qid": "MB44", "query": "Palestinian rights"},
        {"qid": "MB45", "query": "Refugees from Gaza"},
    ])

# Charger les qrels proprement (supporte espaces multiples comme sÃ©parateurs)
def charger_qrels():
    qrels = pd.read_csv("qrels.tsv", delim_whitespace=True, header=None, names=["qid", "iter", "docno", "label"])
    return qrels

# Fonction dâ€™Ã©valuation
def evaluer_modeles_etendus():
    topics_df = charger_requetes()
    qrels_df = charger_qrels()

    print("\nâœ” QIDs dans topics :", topics_df['qid'].unique())
    print("âœ” QIDs dans qrels :", qrels_df['qid'].unique())

    for index_name, index_path in [
        ('Original', "./tweet_index"),
        ('StemmisÃ©', "./index_stem"),
        ('LemmatisÃ©', "./index_lemma")
    ]:
        print(f"\n====================\nðŸ”Ž Ã‰valuation de lâ€™index : {index_name}")

        # Charger l'index
        index_ref = pt.IndexFactory.of(index_path)

        # DÃ©finir les modÃ¨les
        models = {
            "BM25": pt.BatchRetrieve(index_ref, wmodel="BM25"),
            "TF-IDF": pt.BatchRetrieve(index_ref, wmodel="TF_IDF"),
            "PL2": pt.BatchRetrieve(index_ref, wmodel="PL2"),
            "DLH": pt.BatchRetrieve(index_ref, wmodel="DLH"),
            "DFRee": pt.BatchRetrieve(index_ref, wmodel="DFRee"),
            "DirichletLM": pt.BatchRetrieve(index_ref, wmodel="DirichletLM"),
            "DFIZ": pt.BatchRetrieve(index_ref, wmodel="DFIZ"),
            "LGD": pt.BatchRetrieve(index_ref, wmodel="LGD"),
            "BM25 (k1=0.9,b=0.3)": pt.BatchRetrieve(index_ref, controls={"wmodel": "BM25", "bm25.k1": 0.9, "bm25.b": 0.3}),
        }
        # Ã‰valuation pour chaque modÃ¨le
        results = []
        for model_name, model in models.items():
            print(f"\nðŸ“Œ ModÃ¨le : {model_name}")

            # Obtenir les rÃ©sultats
            results = model.transform(topics_df)

            # Afficher les 3 premiers rÃ©sultats
            print("Top 3 rÃ©sultats :")
            print(results[['qid', 'docno', 'score']].head(3))

            # Ã‰valuer le modÃ¨le avec qrels
            try:
                eval = pt.Experiment(
                    [model],
                    topics_df,
                    qrels_df,
                    eval_metrics=["MAP", "P@1", "P@5", "P@10", "R-P"]
                )
                print("\nðŸ“Š RÃ©sultats dâ€™Ã©valuation :")
                print(eval)
                results.append(eval)
            except ValueError as e:
                print(f"âš  Erreur dâ€™Ã©valuation : {e}")

        # Afficher les statistiques de l'index
        # Combiner les rÃ©sultats
        final_results = pd.concat(results)
        final_results.to_csv('evaluation_results.csv', index=False)

        print("RÃ©sultats d'Ã©valuation:")
        print(final_results)
        
        stats = index_ref.getCollectionStatistics()
        print(f"\nðŸ“š Statistiques de lâ€™index {index_name} :")
        print(f"- Documents : {stats.getNumberOfDocuments()}")
        print(f"- Termes uniques : {stats.getNumberOfUniqueTerms()}")
        print(f"- Longueur moyenne : {stats.getAverageDocumentLength():.2f}")

# Lancer lâ€™Ã©valuation
evaluer_modeles_etendus()
print("\nâœ… Ã‰valuation complÃ¨te terminÃ©e.")
